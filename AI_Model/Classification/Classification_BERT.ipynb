{"cells":[{"cell_type":"code","source":["# pip install konlpy"],"metadata":{"id":"_obZaqpS5Fu2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pip install scikit-learn"],"metadata":{"id":"iVuVzUq-90PV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pip install gensim scikit-learn"],"metadata":{"id":"thKxe0199mqT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from gensim.models import Word2Vec\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report\n","import numpy as np"],"metadata":{"id":"qnhMJ_Oz98sx"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"63s3Oxls_FtV"},"outputs":[],"source":["# 필요한 라이브러리를 임포트\n","import os\n","from xml.etree import ElementTree\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report, accuracy_score\n","from sklearn.pipeline import Pipeline\n","import os\n","from xml.etree import ElementTree\n","import json\n","\n","# 폴더 내의 모든 파일 경로를 가져오는 함수\n","def get_file_paths(directory):\n","    file_paths = [os.path.join(directory, filename) for filename in os.listdir(directory) if os.path.isfile(os.path.join(directory, filename))]\n","    return file_paths\n","\n","def extract_bsisFacts_from_folder(folder_path):\n","    bsisFacts_list = []  # 'bsisFacts' 값을 저장할 리스트\n","\n","    # 지정된 폴더 내의 모든 파일 목록을 얻기 위해 os.listdir()를 사용\n","    for filename in os.listdir(folder_path):\n","        file_path = os.path.join(folder_path, filename)  # 파일 경로 생성\n","        if os.path.isfile(file_path) and filename.endswith('.json'):\n","            # 파일인지 확인하고 확장자가 '.json'인 파일만 처리\n","            with open(file_path, 'r', encoding='utf-8') as file:\n","                content = json.load(file)\n","                if 'facts' in content and 'bsisFacts' in content['facts']:\n","                    # 'bsisFacts'가 'facts' 안에 존재하는 경우\n","                    bsisFacts = content['facts']['bsisFacts']\n","                    bsisFacts_list.extend(bsisFacts)  # 리스트 확장\n","\n","    return bsisFacts_list\n","\n","\n","# 폴더 경로 지정\n","civil_case_directory = '/content/drive/MyDrive/SKT_FLY_AI/Project/civil/'  # 민사 사건 파일이 있는 폴더 경로\n","criminal_case_directory = '/content/drive/MyDrive/SKT_FLY_AI/Project/criminal/'  # 형사 사건 파일이 있는 폴더 경로\n","\n","# 데이터 로드\n","civil_texts = extract_bsisFacts_from_folder(civil_case_directory)\n","criminal_texts = extract_bsisFacts_from_folder(criminal_case_directory)\n","\n","# 민사 및 형사 텍스트 데이터를 하나의 데이터셋으로 결합\n","texts = civil_texts + criminal_texts  # 전체 텍스트 데이터\n","labels = ['민사'] * len(civil_texts) + ['형사'] * len(criminal_texts)  # 민사: 0, 형사: 1\n","\n","from konlpy.tag import Okt\n","\n","# Okt 객체를 생성합니다.\n","okt = Okt()\n","\n","# 사용자 정의 불용어 리스트를 정의합니다.\n","stop_words = ['는', '의', '에', '을', '를', '과', '와', '이', '가', '도', '으로', '로', '에서', '하다', '형사', '민사', '형법', '민법']\n","\n","# 민사 및 형사 텍스트가 저장된 변수를 정의합니다.\n","# civil_texts = [\"민사 예시 텍스트 데이터\", \"민사 텍스트의 또 다른 예시\"]\n","# criminal_texts = [\"형사 예시 텍스트 데이터\", \"형사 텍스트의 또 다른 예시\"]\n","\n","# 텍스트를 토큰화합니다.\n","civil_tokens = [okt.morphs(text) for text in civil_texts]\n","criminal_tokens = [okt.morphs(text) for text in criminal_texts]\n","\n","# 불용어를 제거합니다.\n","civil_processed = [[word for word in tokens if word not in stop_words] for tokens in civil_tokens]\n","criminal_processed = [[word for word in tokens if word not in stop_words] for tokens in criminal_tokens]\n","\n","# 각 리스트의 단어들을 집합으로 변환합니다.\n","# civil_word_set = set(word for sublist in civil_processed for word in sublist)\n","# criminal_word_set = set(word for sublist in criminal_processed for word in sublist)\n","\n","# 두 집합의 교집합(공통 단어)을 찾습니다.\n","# common_words = civil_word_set.intersection(criminal_word_set)\n","\n","# 공통 단어를 제거합니다.\n","# civil_final = [[word for word in sublist if word not in common_words] for sublist in civil_processed]\n","# criminal_final = [[word for word in sublist if word not in common_words] for sublist in criminal_processed]\n","\n","# 결과를 변수에 저장합니다.\n","civil_texts = [' '.join(tokens) for tokens in civil_processed]\n","criminal_texts = [' '.join(tokens) for tokens in criminal_processed]"]},{"cell_type":"code","source":["print(len(civil_texts))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ibM5VtzbJek0","executionInfo":{"status":"ok","timestamp":1706614816707,"user_tz":-540,"elapsed":308,"user":{"displayName":"김윤서","userId":"14816156544566013232"}},"outputId":"0efcde87-c960-4d34-e2b6-b38ffdfaf3d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["454\n"]}]},{"cell_type":"code","source":["print(civil_texts)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LCow3GI-JK72","executionInfo":{"status":"ok","timestamp":1706614381476,"user_tz":-540,"elapsed":355,"user":{"displayName":"김윤서","userId":"14816156544566013232"}},"outputId":"73e0a0d4-18de-4c1d-b219-ab425d9d9268"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['심 “ 송수 ” “ 송수 ” “ 배후 조정 ” “ 배후 조종 ” 고친다', '심 소외 원고 원고 민주화 명예 회복 심의 심의 민주화 명예 회복 6123 호로 제정 13289 호로 되기 전의 민주화', '소외 28,092,120원 원고 원고 45,617,000원', '심 \"[ 근거 ]\" 환송 민주화 명예 회복 심의 조회', '민사소송법 420조 심판 인용', '원고 태양광 발전 태양광 발전 적합한 가능한지 피고', '부란 가축 사육제 구역', '원고 태양광 발전 소외 474,660,000원 소외 50,000,000원', '원고 문화재 현상 구역 태양광 발전', '원고 50,000,000원', '적용 문화재 현상 구역', '홈페이지 홈페이지 홈페이지 뉴스 『 취재 커지는 입시 비리 의혹 … 폭풍 예고 』 『 취재 플러스 커지는 야구 입시 비리 의혹 』 사무국 원고 업무방해 고소했다', '원고 야구 어기 강압 고교생 했다는 게재', '피고 취재', '변론 게재 홈페이지 뉴스 서비스 였고 웹 에이전시 피고', '피고 뉴스 서비스 웹 에이전시 수반 포괄 진흥 9조 뉴스 서비스', '게재 홈페이지 뉴스 서비스 콘텐츠 허브 였고 웹 에이전시 피고 피고 뉴스 서비스 웹 에이전시 수반 포괄 뉴스 서비스', '투수 이닝 투구 타자 타석 한하여 강화하였다', '원고 소외 사무국 고교야구 소외 소외 왕중 왕', '소외 소외 소외 왕중 왕전 이닝 투구 하였기에 출전', '원고 “ 원하는 원하는 ” 들며 소외 소외 왕중 왕전', '파악 원고 업무방해 강요 고소하였다', '탈 외적 효력 원고 불기소처분 불기소처분 재정신청 재정신청 항고 기각', '피고 원고 야구 어기 강압 했고 그렇게 고교생 합격 원고 업무방해 고소했다', '9월 5일 11일 소외 소외', '왕중 왕전', '요강 따르면 투수 이닝 타자 타석 들어서야', '소외 소외 충족', '소외 소외 성공하였다는 적시하였다', '피고 경마 경마 오고 겨울철 경마장 경주로 트랙 모래 얼어서 경마 지장 생기는 방지 200 t 다량 소금 경주로 살포', '원고 답 2,093 ㎡ 그중 1,046 ㎡ 답 됨 전의 1,935 ㎡ 비닐하우스 농원 농원', '농원 경마 북쪽 양재천 봉담 의왕 도시 화도 피고 경마 개장 상당히 231~361 t 소금', '피고 경마 개장 상당히 231~361 t 소금', '분재 농원 수용 지하수 염소 농도 246~427 ㎎ ℓ 화훼농원 수용 지하수 염소 농도 386~4 087 ㎎ ℓ 농업 용수 수질 한도 경마 지하수 염소 농도 341~345 ㎎ ℓ 상당히 높으며 경마 분재 농원 화훼 농원 천 염소 농도 120~1 400 ㎎ ℓ 이른다', '환경 경마 토양 지하수 시료 채취 경마 경주로 소금 오염 물질 지하수 흐를수록 추정 밝혔다', '분재 화훼 농원 분재 농원', '농작물 따르면 분재 특히 절단 많아서 염소 농도 물이 장기간 고사 추정 화훼 염소 농도 농업 용수 수질 한도 낮아야 증상 나타나는데 수질 개선 이루어지지 90~100% 고사', '원고 1,935 ㎡ 면적 비닐하우스 블루베리 묘목 식재하였다가 나무 생 장하거나 결실 대부분 고사 이르자 블루베리', '감정인 소외 식재 블루베리 나무 생 장하거나 결실 대부분 원고 목록 71,416,000원 (= 실수입 19,200,000원 + 실수입 29,520,000원 + 실수입 18,096,000원 + 블루베리 묘목 식재하는 4,600,000원', '소외 1955년 생 담낭염 피고 입원 담 도배 액술 도관 삽 PTGBD insertion 혈압 저하 고열 패혈증 생기 중환자실 옮겨져 유량 비강 캐뉼 산소 투여', '피고 소외 낙상 도구 매뉴얼 낙상 위험관리 군 낙상 요인 최대한 낮추고 바퀴 사이드 레일 올림 침상 난간 안전벨트 낙상 방지 였고 소외 낙상 방지 주의', '소외 중환자실 뇌손상 입는 낙상 당하였다', '피고 간호 간호사 03:25 소외 뒤척임 안정 수면 TGBD 배액 이었는데 쿵 돌아보니 침상 난간 안전벨트 난간 넘어와 닿아있는 넘어지며 찧는', '피고 중환자실 간호사', '원고 낙상 담금으로 85,165,730원 46,340,280원 35,159,080원', '피고 의료법 소외 소외 망인 소외 진료 원고 망인 유일한 상속인', '망인 심혈관계 일종 심근경색 진단 스 텐트 혈관 조물 시술 심근경색 장기간', '망인 발목 다쳐 13:30 원하였다', '소외 망인 진찰 엑스레이 발목 인대 진단 14:26 주사약 스테로이드 염 진통제 NSAID 일종 디클로페낙 성분 사제 로페낙 ml 디클로페낙 나트륨 먹는 엔클로페낙정 아세 클로 페낙 에페 신정 에페 리손 염 케이 비피드정 레바 미피드 처방', '소외 처방 간호사 소외 망인 유니 페낙 근 육 소외 망인 처방 로페낙 간호사 소외 망인 유니페낙 디클로페낙 나트륨 주성분 스테로이드 염 진통 제로 성분 동일하고 다르다', '망인 맞은 처방약 조제 쪽지 보여주면서 약과 쪽지 적힌 성분 같은지 묻고 성분 비슷하다고 큰일 동행 망인 동거인 소외 되돌아갔다', '망인 발목 다친 여서 걸음 느렸으므로 소외 돌아가 소외 망인 디클로페낙 부작용 망인 소외 있었나 온다고 14:36 응급실', '망인 응급실 경직 호흡 곤란 증세 디클로페낙 과민반응 약물 투여 션 카테 터 삽관술 심폐소생술 기적 심 조율 산소 흡입 처치 16:11 심근경색 과민성 쇼크 의증', '망인 부검 디클로페낙 아나필락시스 Anaphylaxis 쇼크 과민성 쇼크 추정', '소외 망인 천 안지원 과실치사 되었으나 소외 내려졌다', '유니페낙 디클로페낙 주성분 류머티스 관절염 퇴 행성 관절 척추 염 염증 통증 조절 약제', '유니페낙 스테로이드 진통 소염제 부작용 케 드물게 쇼크 증상 흉통 냉 호흡 곤란 사지마 비감 혈압 저하 발진 가려움 장관 증상 소화 궤양 식욕부진 구역 복통 설사 혈액학 과립 감소 혈소판 감소 빈혈 재생 빈혈 용혈 빈혈 경향', '증상 기능 신부전 천식 발작 발진 두드러기 과민증 두통 졸음 현기 불면 신경과민 저림 착란 환각 경련 혈압 저하 심계 항진 빈맥 증상 유발', '유니페낙 부작용 없으나 드물게 아나필락시스 쇼크 과민성 쇼크', '유니페낙 부작용 부작용 위염 위궤양 기능 저하 대부분 약물 투여 적절한 회복 가능한 부작용', '스테로이드 진통 소염제 안전성 연구 유니페낙 760만 보편 약물 개국 스테로이드 소염제 처방 약물 효과 약제 안전성 지닌다고', '디클로페낙 아나필락시스 과민성 쇼크 1974년 되기 국내외 정확한 률 않으나 드물게 추정', '1993년 사우디아라비아 남성 디클로페낙 근육 투여 15분 아나필락시스 쇼크 심폐소생술 학계 2001년 디클로페낙 경구 투여 아나필락시스 쇼크 여아의 증례보고', '아나필락시스 과민반응 전격 예측 불가능하나 약물 아나필락시스 대부분 스테로이드 진통 소염제 항생제 방사선 만큼 약물 처방 물이 동일성 물이 있었다면 성분 체약 처방', '심장질환 아나필락시스 시키지는 않으나 심장질환 아나필락시스 예후 불량할수', '원고 항소 심 청구원인 변제자대위 대위 심 심의', '피고 사용자 에만 기기 되도록 잠금 잠금 기능 설정', '피고 사용자 잊어버린 ID 계정 페이지 사용자 설정 답 설정 사용자 본인 기기 피고 서비스 잠금을', '피고 사용설명서 ID 없거나 암호 설정 없으면 계정 없으며 기기', \"예방 기적 ID 계정 페이지 계정 업데이트 합니다 '. 홈페이지 사용자 신원 ID 잠금을 .’\", '원고 피고 잠금을 피고 원고 ID 원고 영수증 아니하였다는', '행업', '원고 합병 원고 포괄', '원고 000,000 000,000 원고 구분 원고', '피고 피고 치킨 배달 가맹', '피고 피고 미디어', '원고 피고', '원고 피고 코션 닭 캠페인 상반기 캠페인 피고 되도록', '피고 원고 신제품 치킨 치킨 황금 빛 양파 엑스트라 버진 올리브유 버터 닭고기 황금비율 소스 중독 성과 양파 버터 간장 황금 컨셉 치킨 방향 설정 밍', '원고 피고 밍 밍 밍 치킨 바람직하다는', '원고 피고 커뮤니케이션 아이디어 모델 소외 콘티 피고 치킨', '피고 원고 콘티 도입 주인공 등장 감 높일 장면 별로 다이나믹 액티브함 설정 원고 피고 반영 콘티 수정 완성 콘티 콘티 피고', '원고 치킨 콘티 피고 치킨 연기 원고 않겠다는 통지', '원고 피고 피고 교체 소문', '교체 만큼 원고 IMC 캠페인 불가능하다', '따라서 메뉴 밍 불가능하다 통지', '피고 원고 만료 통보 원고 피고 이메일', '피고 피고', '피고 치킨 텔레비전 치킨', '텔레비전 되기 치킨', '원고 이의신청 특허청 상표 출원 피고 동업 원고 상표 알면서 상표 출원 상표 상표 34조 상표 출원', '심 같으므로 민사소송법 420조 인용 심 인용', '설시 고쳐 쓰는 심 같으므로 민사소송법 420조 인용', '01:13 레커 가해 정명 교차로 300 미터 후방 편도 삼거리 방면 무룡산 방면 황색 점멸 정명 교차로 좌회전 지르기 황색 실선 침범', '정명 교차로 무룡 터널 방향 좌회전 측면 가해 앞범퍼 정명 교차로 가드레일 방향 밀려나가면서 측면 가드레일 가드레일', '약도 같고 승자 망인 다발 심 폐정 이르렀다', '날씨 맑고 노면 건조하였으며 교통 애도', '원고 가해 가해 공제사업 망인 유족 배상금 망인 유족', '정명 교차로 구간 피고 피고 가드레일', '국토교통부 고시 지침 지침 방호', '피고 피고', '원고 피고', '피고 아니하겠다는 원고', '피고 운전자 금지 구역 교통상황 열면서 주행 원고 부딪히는', '태우기 일렬 가장자리 줄지어', '운전자 미연 방지 움직임 예의 주시 주의', '원고 피고 피고 BVR 게임 기기 시스템 기기 원고 원고 대리점', '피고 기기 6,000만원', '원고 6,237,000원 피고', '원고 피고 기기 리스 원고 기기 리스', '원고 기기', '원고 원고 피고 피고 쌀 운반 빻 튀긴 자루 담아 옮기는 한과 재료 튀긴 쌀 피고 원고', '피고 가칭 주택조합 피고 택법 16870 호로 되기 전의 택법 11조 조합원 주택조합 결성 비법인사단', '피고 피고 피고 원고 피고 조합원', '피고 면적 38,045 11.81% 92 93 ㎡ 사용권 복리 762 조합원 일간신문 홈페이지 공고', '원고 피고 납 입하였다', '원고 원고 요건', \"원고 근로자 파견 피고 피고 피고 남동구 '(' 파견\", '원고 CNC 알루미늄 스마트폰 볼륨 알루미늄 식히기 99.9% 메탄올 메틸알코올 분사 기계 6~7 완성 빼내는 메탄올 보충 남은 메탄올 에어건 제거', '원고 몸살 기운 눈앞 뿌연 증상 조퇴 잃은 옮겨졌다 원고 저하 시신경 증세 보였다', '원고 시신경염 대사성 병증 재해 질병', '원고 양안 시신경염 시력 저하 양안 원거리 나안 시력 수지 양안 근거리 나안 시력 0.02 불가한', \"원고 근로자 파견 화해 피고 부천시 '(' 파견\", '원고 CNC 알루미늄 스마트폰 볼륨 야간 알루미늄 식히기 99.9% 메탄올 분사 기계 작동 완성 탈착 메탄올 보충 남은 메탄올 에어건 제거', '원고 침침해지고 숨 가빠지는 증상 느꼈고 좋지 조퇴 원고 호흡 곤란 시력 저하 증세 원하였다', '원고 양안 시신경염 재해 질병', '원고 양안 시신경염 시력 저하 우안 원거리 근거리 나안 시력 수동 좌안 원거리 근거리 나안 시력 0.02 불가한', '원고 근로자 파견 화해 피고 파견', '원고 CNC 알루미늄 스마트폰 볼륨 야간', '알루미늄 식히기 99.9% 메탄올 분사 기계 작동 완성 탈착 메탄올 보충 남은 메탄올 에어건 제거', '원고 침침해지고 숨 가빠지는 증상 느꼈고 좋지 조퇴 원고 호흡 곤란 시력 저하 증세 원하였다', '원고 양안 시신경염 재해 질병', '원고 양안 시신경염 시력 저하 우안 원거리 근거리 나안 시력 수동 좌안 원거리 근거리 나안 시력 0.02 불가한', '메탄올 메탄올 보건 규칙 420조 별표 유해 물질 흡입 심한 자극 호흡기 자극 졸음 현기증 일으킬 태아 생식능력 일으킬', '장기간 노 중추신경계 시신경 일으킨다고', '원고 집기 임차권 권리금 권리금', '권리금 식수 인원 1일 원고 추산 식수 인원 변동 권리금 차감 권리금 특약', '피고 권리금 원고 피고 선정당사자 권리금', '피고 원고 권리금 4,800만 6,200만 피고', '피고 식수 인원 당초 차이 권리금 원고 피고 권리금', '원고 476.1 ㎡ 근 린 170만원', '원고 피고 주택조합 피고 55억 받으며 43억 특약', '원고 피고 피고 피고 원고 서에', '원고 피고 마쳐', '원고 원고 여유 준다면 최대한 이른 시일 이주 내용증명', '지나도록 아니하였고 피고 가단 313591 호로', '가단 313591 피고 피고 강제집행', '원고 피고 49,000,000원 29,000,000원 피고 지연손해금', '원고 원고 피고 피고 보조참가인 피고 공제사업', '원고 14:05 방면 동호대교 방향 램프 원고 피고 보조참가인 피고 충돌', '원고 피고 탑승 3,359,290원 피고 탑승 3,058,230원 원고 운전자 1,649,060원 원고 500,000원', '공터 얼어 곤란했는데 가장자리', '피고 음주 피고 일으켰다', '원고 어업 회사법 45,000,000원 45,000,000원 대출받았고 피고 원고 원고', '원고 45,135,368원 45,000,000원 + 135,368원 대위변제', '원고 따르면 원고 완제 지연손해금 원고 법적 원고 법적 205,267원', '피고 목록 1/2 마쳤다가 피고 증여계약 증여계약 피고', '피고 피고 최고 130,000,000원 근 저당권설정계약 근 저당권설정계약 피고 동부 12914 호로 근저당권 설정', '피고 피고 주택조합 피고 피고 동부 4585 호로', '변론 피고 목록 1/2 경매 낙찰', '원고 근로자 재해 산재', '피고 피고 운전자', '근로자 망인 21:40 외곽순환 고속도로 방향 76.18 km 요금소 하이패스 구간 잘못', '통행 요금 수납 하이패스 구간 수납 창구 건너갔다가 요금 수납 되돌아가려고 펜스 넘는 하이패스 구간 제한 최고 속도 시속 km 시속 81 km 피고 당하는 당하였다', '망인 내출혈 주막 족부 내과 중수 골 곡절 다발 좌상 입원', '피고 운전자 제한 속도 준수 주시 방지 의의 올리 시켜 망인', '피고 운전자 제한 속도 준수 주시 방지 의의 올리 시켜 망인', '원고 재해 망인 요양 916,940원 휴업 5,813,040원 망인 유족 유족 시금 65,331,500원 9,539,140원 산재 81,600,620원', '재해 87조 자의 재해 도안 자의 손해배상청구권 대위', '원고 화물자동차 운수 35조 조합원 24:00 24:00 까지로 배상 책임', '약관 화물자동차', '화주 화주 화물 피고', '피고 14:30 피고 적재 팔레트 판용 인쇄회로기판 PCB 적재 팔레트 포장 비스듬히 빠져나오게 팔레트 실린 인쇄회로기판 안전성', '화주 운 송료 16,723,740원 배상', '원고 14,200,000원 1,072,000원', '원고 손해배상 14,051,900원', '원고 피고 50,000,000원 2,000,000원 당사자', '주택 가칭 8.29 원고', '원수 피고', '피고', '피고 원고 220,000,000원 220,000,000원 1,000,000,000원', '원고 피고', '피고 원고 이루어지지 않았다는 통보', '7조 피고 이루어지지 피고 하고있는 피고 원고 이루어지지 않았다는 통보', '피고 10.19 9,245,000,000원 이루어지지', '피고 2,160,000,000원', '피고 통보 ProjectManagement', '비로소 피고 성사 피고 마쳐졌다', '피고 피고 조합원 3,000,000,000원', '원고 피고 60,000,000원 지연 배액 배상 위약금 조항', '에다가 지연손해금 정함 없었던 보태 피고 이행지체 398조 손해배상 위약벌', '39,466 ㎡ 면적 146,139.63 ㎡ 조합원 194 일반 538 68 주택 정비', '부산광역시 68', '부산광역시 81억 부산광역시', '부산광역시 주택 LH 입장 차이 LH 74억 입장 곤란 겪고', '행업 원고', '원고 감정평가 의향 피고 피고', '피고 83억', '피고 피고 원고', '원고 피고 대관 도시 정비', '원고 피고 피고', '가능하도록 정관 부산광역시 조문 39조 용의 정관', '도시 정비', '피고 LH 지나지 반려', '고시 이루어졌다 소유권보존등기 총회 정관 2016.10', '원고 부산광역시 도시 정비 질의 법률자문 예비 입주자', '국토교통부 LH 철회 LH 자지 정이', '입주자 공고 2017.4 입주하였다', '피고 넘겨받았고', '피고 ■ 근 린', '원고 사의 피고', '원고 < > 피고 납 납 입하였다', '원고 피고 이분 의사표시 담긴 내용증명 송달', '사의 원고 기둥 기둥 면적 소화전 과환 풍구', '설계도 기둥 기둥 문구 □ ■ 기둥 면적 나타내고 기둥 용면 파악', '환 풍구 유리 벽면 됨으로써 유리 189 시야 가리고', '피고 기둥 기둥 지는', '기둥 용면 원고 ■ 1.99% (= 기둥 환 풍구 면적 1.2632 ㎡ 면적 63.420 ㎡ 원고 ■ ■ 4.39% (= 기둥 면적 2.646 ㎡ 면적 60.206 ㎡', '원고 ■ ■ 소화전 원고 소화전 잠그면 화재 예방 소방시설 •', '피고 항소 심 다르지 심 에다가 보태 보더 심의', '추가하거나 고치는 심 같으므로 민사소송법 420조 인용', '원고 소외 가합 34729', '위법 소외 원고 631,131,600원 9.13 갚는', '원고 확정판결 집행권 105977 호로 추심 686,290,772원 추심명령 추심명령 추심명령 정본 피고 송달', '소외 피고 536 조합원 담금 피고', '소외 피고 대수 724 늘리고 조합원 담금 합', '반영', '원고 주택', '광역시 중구청 원고 주택 부과', '무상 가능하며 행정재산 폐지 총무 과로 인계 반 유상 태상 착공 불가하오니 착공 소관 부서 총무 행정절차 착공 고시 첨부 하시기 바랍니다', '번지 폐지 유상', '0000,000 번지 공공시설 상당하는 무상', '공공시설 상당하는 무상', '번지 폐지 유상 공공시설 상당하는 무상', '번지 공공시설 구거 상당하는 무상', '원고 폐지 폐지 원고 광역시 중구청 지목 기능', '원고 폐지 착오 무상 폐지 광역시 중구청 기능 명확하다는 원고 폐지 착오 수용 없다고 회신', '원고 폐지 무산 피고 롤 1,048,633,000원', '였는데 원고 피고 Purchase Order', '전기 충전 무선 완제 품 배터리 TX 모듈 어댑터 5,000 188,920,000원 (= 5,000 37,784원', '어댑터 배터리 팩 PBA Panel Board Assembly PBA', '피고 원고 어댑터 2,500 어댑터 2,500 어댑터 5,000 ㉡ 배터리 팩 5,000', '피고 PBA 5,000 원고 5,940만', '원고 피고 이메일 도안 매뉴얼', '피고 원고 어댑터 배터리 팩 첨부 견적 어댑터 배터리 팩 참조 원고 TX Board 물량용 무선 완제 품 회신 매뉴얼', '원고 PBA 5,000 피고 피고', '원고 포장 매뉴얼 피고 로고 새겼다', 'PBA Qi 요건 충족 충전', 'Qi 무선 충전 기술 보편 기술 WPC Wireless Power Consortium 기술 i WPC 제정 무선 충전 기술 Qi 적합 프로그램', 'Qi 않더라도 Qi 규격 무선 충전 가능한 즉 Qi 호환 가능하다', '피고 원고 비즈니스 희망 원고 모델 무선 원고 비즈니스 비즈니스 성사 케이블 피고', '⑦ 피고 PBA 피고 심 수원지방법원 가단 43079 피고 항소 심 패소 수원지방법원 2017451280 고심 계류 대법원 213385', '심판 피고 PBA Qi 않더라도 Qi 따르도록 PBA Qi 충족 피고', '원고 피고 피고 견적 피고 견적 피고', '피고 PBA 통보 무선 시료 회로 정이 필요하다고 알렸다 원고 통보', '피고 매뉴얼 인쇄 보냈으며 추가물량 견적 전계 무선 기기 모델 전파 연구원 전파 58조 방송통신 적합 필증', '원고 무선 포장재 송부하 피고 가인 출고 인상 반영 원고 피고 포장재 인상', '피고 무선 피고 4,970 화물 가능하다고 알려주었다', '피고 근 린 가함', '사의 피고 658,100,000원 피고 65,810,000원 도금 164,525,000원 164,525,000원', '원고 피고 피고 263,240,000원', '15901 ㎡ 15901분 2816 2015.2 마쳐졌다가 피고 가액 9,856만', '피고 HHHHH 협동조합 최고 8,400만 근저당권 설정 목록 합', '원고 원고 사해행위 피고 근저당권 설정 원상회복 가액 배상', '제소 원고 < > 같고', '444,659,298원 명확하다', '9,081 ㎡ 택지 편입 보상금 1,439,079,960원', '9,910 ㎡', '등본 근저당권 피고 최고 근저당권 근저당권 피고 최고 근저당권 근저당권 피고 최고 일본국 법화 3,200만 엔 근저당권 설정 근저당권 근저당권', '특약 부등 12-4 17-2 18-2 피고 열거 근저당권 설정 말소 민 법적 책임 피고', '', '피고 이는 피고 근저당권 말소', '수용 보상금 양도소득세 원고 양도소득세 333,128,340원', '양도소득세 원고 95,483,086원 .), 95,474,760원 양도소득세', '양도소득세 647,818,920원', '피고 주택 정비 정비', '면적 면적 ㎡ 9,380원 Py 31,000원 )( 면적 면적 ㎡ 9,380원 Py 31,000원 곱 정비 구역 면적 적용 가시 면적 적용 2조 당사자', '3조 수탁 성실히', '효력', '피고 촉구 내용증명 30일 내용증명', '피고 되었다고 통보', '원고 황해 680 집행력 정본 11663 호로 피고 680,000,000원 추심명령 추심명령 추심명령 피고 송달', '원고 수원지방법원 오산시 307 집행력 정본 4628 호로 피고 715,397,260원 추심명령 추심명령 추심명령 피고 송달', '추심명령 전인 피고 890,298,723원 원고 채권양도 채권양도 피고 통지', '원고 234,000,000원', '피고 중개', '누수 되기', '원고 피고 원고 피고 2,550,000,000원', '원고 피고 원고 피고 2,500,000,000원', '지급시 8%', '원고 1,900,000,000원 빌려주고 피고', '', '3%', '잃었을 완제 이르기까지 8% 연체', '원고 피고 2,550,000,000원 2,500,000,000원 1,900,000,000원 만료', '원고 주권 의결권 피고', '원고 피고 원고 피고 1,900,000,000원', '원고 피고 11월', '피고 원고 6,950,000,000원 (= 2,550,000,000원 + 2,500,000,000원 + 1,900,000,000원 3,992,290', '독립 당사자 참가인 참가인 피고 2,468,200 원고 원고 피고 명의신탁 피고 귀속 증여세 가산 합 2,246,227,530원 부과 과세 과세', '원고 도시 환경 비법 정비', '피고 석남동 491-3 주택 정비 도시 환경 비법 주택 정비 피고', '피고 정비 공고 내고 원고 정비 피고 원고 정비', '원고 정비 피고 않겠습니다 피고', '원고 피고 피고 피고 시공사 시공사 공자 20일', '원고 피고 피고 피고 시공사 시공사 공자 20일', '원고 피고 피고 피고 시공사 시공사 피고 날로 10일', '원고 피고 피고 원고 에게로 되거나 피고 원고 금은', '피고 정비 구역 인천광역시 도시 심의 심의 용적률 650% 부결 원고 피고 피고 임원 합의해지 피고 원고', '원고 피고 원고 파견 피고 파견 철수', '피고 총회 원고 원고 통지', '원고 피고 피고 귀책사유 지연 가합 7041 전소 되었다고 어렵다는 원고 기각', '피고 전소 원고 유효하니 원활한 원고', '피고 자력 조합원 72% 징 구한 총회 창립총회 파견 원고 피고 수용 없다고 회신', '피고 원고 원고 파견 협조 원고', '원고 정비 도시 환경 비법 73조 피고 통지', '피고 행정청 원고 정비 원고 원고 불가능하게 통보 이는 원고', '원고 피고 지연 원고 정비 피고 지연 지연 되거나 소장 부본 송달 의사표시 이는 피고', '면적 24,945 평이 피고 진단', '김포시 12,298 ㎡ 지번 만으로 등장 있었는데 1950 장남 상속 대부분', '장남 피고 삼남 피고 피고 두었는데 1965 상속인 상속 대부분 상속 상속', '마쳐졌다가 원고 피고 마쳐졌다', '1971 특별 9153 호로 폐지 근거 원고 마쳐졌다가 1986 1986 피고 마쳐졌다', '원고 따로 살면서 왕래 1994 피고 정서 보증인', '김포시 9,674 ㎡ 2,624 ㎡ 116 ㎡ 1,882 ㎡ 7,676 ㎡', '7,400만 원고 피고 수기 피고', '영수증 피고 원고 쓰여 피고 원고 피고', '원고 정확한 시기 피고', '김포시 원고 보상금 503,121,330원 피고 10,416원 피고 464,410,914원', '원고 피고', '원고 피고 보상금 668,664,880원', '피고 피고 상속 상속', '영수증 도인 피고 만이 피고 피고', '피고', '1999 피고 5,355 ㎡ 최고 근저당권 설정 근저당권', '퇴사 다가 소외 소외', '소외 귀속 237,608,750원 소득세 207,317,900원 소득세 2,084,900원 11월 소득세 866,090원 447,877,640원 국세', '납세의무 까지로 납세의무 정통 지를 건의 소득세 납세의무 까지로 납세의무 정통 지를', '가공업 132,686,670원', '국세 근거 피고 경매 개시', '경매 근저당권 피고 106,156,041원 원고 피고 이의', '원고 피고 근저당권 근저당권 통정 허위표시 무효 이므로 근저당권 말소 구한 동부 가단 000000 호로 근저당권 소의', '숙모 피고 원고 기각', '원고 피고 피고', '피고 원고 80,000,000원', '피고 5,555만원 소외 피고 피고 출하도록 3,955만원 피고', 'JJJ 00시 00-0 429,000,000원 양도소득세 안양세무서 납기 양도소득세 95,358,500원 JJJ 양도소득세 가산금 124,538,150원', 'JJJ 비사 위인 피고 목록', 'JJJ 피고 피고 설정 근저당권 근저당권 5,600만 갈음', '임대차보호법 3조 대항력 갖추고', '피고 근저당권 말소', 'JJJ', '변론', '고속도로 방면 278.1 km 방향', '교통사고 실황조사서 고속도로 호남 고속도로 분기점 고속도로 방면 방면 호남 고속도로 유성 방면 방향 방면 방향 켜지 장애물 가로질러 뒤늦게', '후방 관광버스 방면 편도 시속 km h 속도 직진', '주시 게을리 뒤늦게 미처 제동장치 조작 과대 조작 과실 급회전 방호벽 기울면서 갓길 가드레일', '73 중증 뇌손상 51 중증 개방 59 중증 다발 개방 뇌손상 이르렀 탑승', '원고 피고 전세버스 공제사업', '원고 19,213,240원 환 피고 66,836,056원 을구상금으로', '4238 교통사고처리특례법 고속도로 호남 고속도로 분기점 방면', '호남 고속도로 방면 방향 켜지 장애물 가로질러 뒤늦게 과실 후방 살피는 주의 기울이지 무턱 과실 케 하였음 항소 됨', '4248 교통사고처리특례법 주시 게을리 뒤늦게 미처 제동장치 조작 과대 조작 과실 심 됨', '원고 피고 1534 ㎡ 2/8 원고 피고 특약 도세 과세 된다면 피고 원고 행각', '원고 근저당권 4,990만 피고', 'AAA CCC BBB CCC', 'CCC DD 1,917 ㎡ ㎡ 199 ㎡ 0000원', '원고 CCC 양도소득세 귀속 양도소득세 0000원 CCC 가산금 0000원 더한 양도소득세 0000원 (= 양도소득세 + 가산금 양도소득세 가산금 0000원', 'CCC DD ㅇㅇㅇㅇ (= + + + + + +', 'CCC 피고 BBB CCC 피고 BBB 631 쌍방 간주', 'CCC 연습장 피고 CCC 연습장', '피고 BBB EEE FFF CCC 외도 혼외자 FFF', 'CCC 피고 BBB CCC 부모님 에게서 분가 CCC 피고 BBB', 'CCC 피고 BBB 쌍방 간주', '피고 BBB 피고 AAA 부양', '변론 원고 피고 1,140,000,000 80,000,000원 1,060,000,000원 원고 피고 마쳐', '피고 사위 원고 273,805,950원', '피고 본인 유일한 답 ㎡ 7,000만원 피고', '근저당권 설정 근저당권 최고 2,900만 5,700만 피고 근저당권 설정 말소 최고 2,000만원 근저당권 설정', '원고 소외 소외 소득세 102,493,180원', '소외 과세 고서 표준 대차대조표 인정이 조정 명세서 임원 단기 377,831,698원 피고 되어있다', '377,831,698원 228,136,167원 소외 피고 피고', '서초세무서 피고 원고 소외 소외 피고 국세 통지 피고 송달', '서초세무서 피고 최고', '원고 자동화 피고 원고 피고 피고 원고 보증계약 피고 신원보증', '피고 원고 주시 게을리 과실 입혔다', '원고 나가는데 없자 일천 피고 태우고 가던', '원고 운전자 연령 한정 특약 피고 적용', '피고 1354 교통사고처리특례법 고심 쳐', '특약 88,334,760원', '원고 피고 구상권 가단 55238 상금 원고 피고 70,667,808원 지연손해금 원고 22,067,233원', '원고 피고 손해배상', '가단 10761 원고 피고 조정', '조정 원고 2016.6 2016.12', '원고 도시 환경 비법 56 8.140 ㎡ 구역 내의 건축물 대지 건축물 도시 환경 개선 조합원 안정 질적 향상 이바지 입니다 참조', '피고 소외 331 ㎡ 슬래브 지붕 주택 합니다 예정자 가칭 주택조합 소외 마쳐주었습니다', '원고 소외 피고 1.000만 20일', '7.000만 하였습니다 합니다', '원고 피고 4516 호로 피고 7.000만 하였습니다', '원고 피고 54496 원고 되었습니다 54496 참조', '피고 근 저당권설정계약 21073 호로 근저당권 최고 112,000.000원 근저당권 설정 마쳤고 근저당권 경매 개시 경매 되었습니다', '근저당권 말소 경매 원고 근저당권 하였습니다', '원고 소외 근저당 말소 140,674,508 참조 피고 82.885 329원 대위변제 하였습니다 대위변제 참조', '원고 도표', '피고 위자료 피고 목록 1/2', '수원지방법원 용인 166198 호로 증여 피고', '이루어진 최고 × 1/2 유일하고 근저당 57,528,922원', '피고 근저당 갚았다 4분 국토해양부 평균', '피고 주택조합 피고 북구 주택 설 주택 주택조합 피고 피고 주택', '원고 피고 북구 2,119 ㎡ 북구 답 2,523 ㎡', '피고 북구 2,119 ㎡ 2,403,700,000원 피고', '피고 북구 답 2,523 ㎡ 2,862,000,000원 피고', '심 유란 같으므로 민사소송법 420조 인용', '원고 피고 피고 피고 소제 동부 가합 9524 둥 원고 피고 움', '원고 피고 변론기일 준비서면 조회 증거신청 변론', '440,844,327원 갚는', '심 심 항소 기각 기각 피고 심 원고', '피고 동부 카단 4836 피고 각기 150,000,000원', '피고 동부 7206 추심명령 150,000,000원 388,627,221원 538,627,221원 추심명령', '추심명령 피고 추십 추십 추심명령 하지도', '', '피고 동부 가합 108637 호로 인격 인격 동일성 계류', '원고 한도 까지로 배상 책임', '피고 심 피고 굴삭기 굴삭기 배상 기계', '주택 굴삭기', '굴삭기 조종 조종 부주의 콘크리트 덩어리 비계 비계 무너지면서 이웃 주택 담장 주택 이웃 주택 경계 담장 담장 담장', '호수 굴삭기', '담장 이웃 주택 담장 5,960,000원 렛츠고 투어 가액 16,714,091원', '원고 배상 책임 배상금 22,370,000원 = 22,670,000원 (= 5,960,000원 + 16,714,091원 1000원 버림', '약관 8조 배상 하나로 케이블 도관 지반 침하 생긴 붕괴 도괴 생긴', '피고 태풍 낙하 알림', '적절한 조취', '피고 주택 원고', '피고 개별 날짜', '피고 원고 6,600,000원 2012.3 50,000,000원 40,000,000원 합 126,600,000원', '피고 주택조합 면적 127,827 ㎡ 피고 면적 5,358 ㎡ 면적 133,185 ㎡', '원고 기계 휴지 배상 책임 배상 책임', '156,200,000원', '피고 도급 배상 책임 배상 책임 배상 책임', '16:12 벽면 녹화 시스템 작동 물이 넘치는 조경수 배관 배관 밸브 조작 조경수 긴급 차단', '조경수 차단 모르고 조경수 차단 PS 조경수 배관 손잡이 배관 밸브 밸브 도구 조작 과실 밸브 개방', '밸브 배관 조경수 배관 배관 말단 부가 캡 마감 노출 였고 밸브 개방 다량 물이 누 물이 분사', '입점 17,990,000원 원고 기하 17,990,000원 17,890,000원', '생 터미널 근 가던 피고 부딪쳐 보 닛 넘어져 회전 근 근육 힘줄 입었다', '입원 진 진료 2,864,050원 637,560원 원고 2,226,490원', '원고 표지 원고 원고', '원고 망인 1964년 창업 이래 원고 이르기까지 55년', '원고 표지 원고 창업 55년 원고 표지 표지 칭할 표지', '원고 메뉴 두툼 갈비 칼집 내어 운 생갈 비구 양념 갈비 구이', '원고 볼록하게 솟고 구멍 불판 숯불 갈비 구워 오목하고 둥글게 파인 불판 가장자리 갈비 양념 부어 감자 사리', '원고 무쇠 불판 요리 감자 사리', '피고 피고 피고', '피고 메뉴 생갈 비구 양념 갈비 구이', '피고 갈비 구이 오목하고 둥글게 파인 불판 가장자리 감자 사리']\n"]}]},{"cell_type":"code","source":["# 필요한 라이브러리를 불러옵니다.\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n","import torch\n","from sklearn.model_selection import train_test_split\n","from konlpy.tag import Okt\n","import os\n","import json"],"metadata":{"id":"hajdG5A-E_5J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# device 설정 (CUDA가 사용 가능한 경우 GPU 사용, 그렇지 않으면 CPU 사용)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n"],"metadata":{"id":"b4c3CcAqE4dc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ... 여기에 파일 로딩 및 전처리 코드를 삽입 ...\n","\n","# 전처리된 텍스트 데이터와 레이블을 결합합니다.\n","texts = civil_texts + criminal_texts  # 전체 텍스트 데이터\n","labels = [0] * len(civil_texts) + [1] * len(criminal_texts)  # 민사: 0, 형사: 1\n","\n","# BERT 토크나이저를 로드합니다.\n","model_name = 'bert-base-multilingual-cased'\n","tokenizer = BertTokenizer.from_pretrained(model_name)\n","\n","# BERT에 맞게 데이터를 인코딩합니다.\n","encoded_data = tokenizer.batch_encode_plus(\n","    texts,\n","    add_special_tokens=True,\n","    return_attention_mask=True,\n","    pad_to_max_length=True,\n","    max_length=256,\n","    return_tensors='pt'\n",")\n","\n","input_ids = encoded_data['input_ids']\n","attention_masks = encoded_data['attention_mask']\n","labels = torch.tensor(labels)\n","\n","# 데이터셋을 생성합니다.\n","dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","# 데이터를 훈련 세트와 검증 세트로 분리합니다.\n","train_size = int(0.8 * len(dataset))\n","val_size = len(dataset) - train_size\n","\n","train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n","\n","# 데이터 로더를 생성합니다.\n","batch_size = 4\n","\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    sampler = RandomSampler(train_dataset),\n","    batch_size = batch_size\n",")\n","\n","validation_dataloader = DataLoader(\n","    val_dataset,\n","    sampler = SequentialSampler(val_dataset),\n","    batch_size = batch_size\n",")\n","\n","# BERT 분류 모델을 로드합니다.\n","model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","\n","# 옵티마이저를 설정합니다.\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n","\n","# 훈련 루프를 정의합니다.\n","epochs = 3\n","# 모델을 device로 보냅니다.\n","model.to(device)\n","for epoch in range(0, epochs):\n","    # 훈련 단계\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        batch = tuple(b.to(device) for b in batch)\n","        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n","        model.zero_grad()\n","        outputs = model(**inputs)\n","        loss = outputs[0]\n","        total_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # 평가 단계\n","    model.eval()\n","    for batch in validation_dataloader:\n","        batch = tuple(b.to(device) for b in batch)\n","        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","            loss = outputs[0]\n","            logits = outputs[1]\n","        # 여기서 logits를 사용하여 정확도 등을 계산할 수 있습니다.\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_afCR1k8Ahpy","executionInfo":{"status":"ok","timestamp":1706614915068,"user_tz":-540,"elapsed":79255,"user":{"displayName":"김윤서","userId":"14816156544566013232"}},"outputId":"111f5a38-98de-448f-a14b-4296521309ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","\n","# 훈련 루프를 정의합니다.\n","epochs = 3\n","for epoch in range(epochs):\n","    # 훈련 단계\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        batch = tuple(b.to(device) for b in batch)\n","        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n","        model.zero_grad()\n","        outputs = model(**inputs)\n","        loss = outputs.loss\n","        total_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # 평가 단계\n","    model.eval()\n","    val_loss = 0\n","    predictions, true_labels = [], []\n","    for batch in validation_dataloader:\n","        batch = tuple(b.to(device) for b in batch)\n","        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","\n","        loss = outputs.loss\n","        logits = outputs.logits\n","        val_loss += loss.item()\n","\n","        # CPU로 데이터 이동\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = inputs['labels'].to('cpu').numpy()\n","\n","        # 예측 값 저장\n","        predictions.append(logits)\n","        true_labels.append(label_ids)\n","\n","    # 평가 결과 계산\n","    predictions = np.concatenate(predictions, axis=0)\n","    true_labels = np.concatenate(true_labels, axis=0)\n","    predicted_labels = np.argmax(predictions, axis=1)\n","    accuracy = accuracy_score(true_labels, predicted_labels)\n","\n","    print(f'Epoch {epoch + 1}/{epochs}')\n","    print(f'Training loss: {total_loss / len(train_dataloader)}')\n","    print(f'Validation loss: {val_loss / len(validation_dataloader)}')\n","    print(f'Accuracy: {accuracy}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JzM9aN_OFyVp","executionInfo":{"status":"ok","timestamp":1706614999129,"user_tz":-540,"elapsed":73098,"user":{"displayName":"김윤서","userId":"14816156544566013232"}},"outputId":"68768935-c795-4489-c472-30436c9b580f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","Training loss: 0.023832691336448643\n","Validation loss: 0.05388624414037032\n","Accuracy: 0.9797297297297297\n","Epoch 2/3\n","Training loss: 0.024252557956552245\n","Validation loss: 0.08928470295772108\n","Accuracy: 0.9695945945945946\n","Epoch 3/3\n","Training loss: 0.015681076579337084\n","Validation loss: 0.07445759098265774\n","Accuracy: 0.972972972972973\n"]}]},{"cell_type":"code","source":["import os\n","import json\n","import torch\n","import numpy as np\n","from transformers import BertTokenizer, BertForSequenceClassification\n","\n","# 폴더 내의 모든 파일 경로를 가져오는 함수\n","def get_file_paths(directory):\n","    file_paths = [os.path.join(directory, filename) for filename in os.listdir(directory) if os.path.isfile(os.path.join(directory, filename))]\n","    return file_paths\n","\n","# 'bsisFacts' 값을 추출하는 함수\n","def extract_bsisFacts_from_folder(folder_path):\n","    bsisFacts_list = []\n","    for filename in os.listdir(folder_path):\n","        file_path = os.path.join(folder_path, filename)\n","        if os.path.isfile(file_path) and filename.endswith('.json'):\n","            with open(file_path, 'r', encoding='utf-8') as file:\n","                content = json.load(file)\n","                if 'facts' in content and 'bsisFacts' in content['facts']:\n","                    bsisFacts = content['facts']['bsisFacts']\n","                    bsisFacts_list.extend(bsisFacts)\n","    return bsisFacts_list\n","\n","# 불용어 처리를 수행하는 함수\n","def remove_stopwords(text, stop_words):\n","    tokens = text.split()\n","    filtered_tokens = [word for word in tokens if word not in stop_words]\n","    return ' '.join(filtered_tokens)\n","\n","# 폴더 경로 지정\n","civil_case_directory = '/content/drive/MyDrive/SKT_FLY_AI/Project/civil/'  # 민사 사건 파일이 있는 폴더 경로\n","criminal_case_directory = '/content/drive/MyDrive/SKT_FLY_AI/Project/criminal/'  # 형사 사건 파일이 있는 폴더 경로\n","\n","# 데이터 로드 및 전처리\n","civil_texts = extract_bsisFacts_from_folder(civil_case_directory)\n","criminal_texts = extract_bsisFacts_from_folder(criminal_case_directory)\n","\n","# 사용자 정의 불용어 리스트 정의\n","stop_words = ['는', '의', '에', '을', '를', '과', '와', '이', '가', '도', '으로', '로', '에서', '하다', '형사', '민사', '형법', '민법']\n","\n","# 불용어 제거\n","civil_texts = [remove_stopwords(text, stop_words) for text in civil_texts]\n","criminal_texts = [remove_stopwords(text, stop_words) for text in criminal_texts]\n","\n","texts = civil_texts + criminal_texts  # 전체 텍스트 데이터\n","labels = ['민사'] * len(civil_texts) + ['형사'] * len(criminal_texts)  # 민사: 0, 형사: 1\n","\n","# BERT 토크나이저와 모델 로드\n","model_name = 'bert-base-multilingual-cased'\n","model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","tokenizer = BertTokenizer.from_pretrained(model_name)\n","\n","# device 설정\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# 파일에서 텍스트 데이터를 불러오고 전처리\n","new_example_file_path = '/content/drive/MyDrive/SKT_FLY_AI/Project/86가합493.json'\n","pred_texts = []\n","with open(new_example_file_path, 'r', encoding='utf-8') as file:\n","    new_example_text = json.load(file)\n","    if 'facts' in new_example_text:\n","        # 'facts' 부분만 문자열로 변환하여 추가\n","        facts_text = json.dumps(new_example_text['facts'], ensure_ascii=False)\n","        pred_texts.append(remove_stopwords(facts_text, stop_words))\n","\n","# BERT 토크나이저를 사용하여 데이터를 인코딩\n","encoded_data = tokenizer.batch_encode_plus(\n","    pred_texts,\n","    add_special_tokens=True,\n","    return_attention_mask=True,\n","    pad_to_max_length=True,\n","    max_length=256,\n","    return_tensors='pt'\n",")\n","\n","input_ids = encoded_data['input_ids'].to(device)\n","attention_mask = encoded_data['attention_mask'].to(device)\n","\n","# 모델을 사용하여 예측\n","model.eval()\n","with torch.no_grad():\n","    outputs = model(input_ids, attention_mask=attention_mask)\n","    logits = outputs.logits\n","\n","# 예측된 레이블을 가져옴\n","predicted_label = torch.argmax(logits, dim=1).cpu().numpy()[0]\n","\n","# 예측된 분류 결과 출력\n","class_names = ['민사', '형사']\n","print(\"예측된 분류:\", class_names[predicted_label])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bPsQ6MiUNBwO","executionInfo":{"status":"ok","timestamp":1706615389214,"user_tz":-540,"elapsed":3467,"user":{"displayName":"김윤서","userId":"14816156544566013232"}},"outputId":"42f4f33c-cbf3-41b2-c33a-85fe0a2f8ae2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["예측된 분류: 민사\n"]}]},{"cell_type":"code","source":["import os\n","import json\n","import torch\n","import numpy as np\n","from transformers import BertTokenizer, BertForSequenceClassification\n","\n","# 폴더 내의 모든 파일 경로를 가져오는 함수\n","def get_file_paths(directory):\n","    file_paths = [os.path.join(directory, filename) for filename in os.listdir(directory) if os.path.isfile(os.path.join(directory, filename))]\n","    return file_paths\n","\n","# 'bsisFacts' 값을 추출하는 함수\n","def extract_bsisFacts_from_folder(folder_path):\n","    bsisFacts_list = []\n","    for filename in os.listdir(folder_path):\n","        file_path = os.path.join(folder_path, filename)\n","        if os.path.isfile(file_path) and filename.endswith('.json'):\n","            with open(file_path, 'r', encoding='utf-8') as file:\n","                content = json.load(file)\n","                if 'facts' in content and 'bsisFacts' in content['facts']:\n","                    bsisFacts = content['facts']['bsisFacts']\n","                    bsisFacts_list.extend(bsisFacts)\n","    return bsisFacts_list\n","\n","# 불용어 처리를 수행하는 함수\n","def remove_stopwords(text, stop_words):\n","    tokens = text.split()\n","    filtered_tokens = [word for word in tokens if word not in stop_words]\n","    return ' '.join(filtered_tokens)\n","\n","# 폴더 경로 지정\n","civil_case_directory = '/content/drive/MyDrive/SKT_FLY_AI/Project/civil/'  # 민사 사건 파일이 있는 폴더 경로\n","criminal_case_directory = '/content/drive/MyDrive/SKT_FLY_AI/Project/criminal/'  # 형사 사건 파일이 있는 폴더 경로\n","\n","# 데이터 로드 및 전처리\n","civil_texts = extract_bsisFacts_from_folder(civil_case_directory)\n","criminal_texts = extract_bsisFacts_from_folder(criminal_case_directory)\n","\n","# 사용자 정의 불용어 리스트 정의\n","stop_words = ['는', '의', '에', '을', '를', '과', '와', '이', '가', '도', '으로', '로', '에서', '하다', '형사', '민사', '형법', '민법']\n","\n","# 불용어 제거\n","civil_texts = [remove_stopwords(text, stop_words) for text in civil_texts]\n","criminal_texts = [remove_stopwords(text, stop_words) for text in criminal_texts]\n","\n","texts = civil_texts + criminal_texts  # 전체 텍스트 데이터\n","labels = ['민사'] * len(civil_texts) + ['형사'] * len(criminal_texts)  # 민사: 0, 형사: 1\n","\n","# BERT 토크나이저와 모델 로드\n","model_name = 'bert-base-multilingual-cased'\n","model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","tokenizer = BertTokenizer.from_pretrained(model_name)\n","\n","# device 설정\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# 파일에서 텍스트 데이터를 불러오고 전처리\n","new_example_file_path = '/content/drive/MyDrive/SKT_FLY_AI/Project/2010고단3085.json'\n","pred_texts = []\n","with open(new_example_file_path, 'r', encoding='utf-8') as file:\n","    new_example_text = json.load(file)\n","    if 'facts' in new_example_text:\n","        # 'facts' 부분만 문자열로 변환하여 추가\n","        facts_text = json.dumps(new_example_text['facts'], ensure_ascii=False)\n","        pred_texts.append(remove_stopwords(facts_text, stop_words))\n","\n","# BERT 토크나이저를 사용하여 데이터를 인코딩\n","encoded_data = tokenizer.batch_encode_plus(\n","    pred_texts,\n","    add_special_tokens=True,\n","    return_attention_mask=True,\n","    pad_to_max_length=True,\n","    max_length=256,\n","    return_tensors='pt'\n",")\n","\n","input_ids = encoded_data['input_ids'].to(device)\n","attention_mask = encoded_data['attention_mask'].to(device)\n","\n","# 모델을 사용하여 예측\n","model.eval()\n","with torch.no_grad():\n","    outputs = model(input_ids, attention_mask=attention_mask)\n","    logits = outputs.logits\n","\n","# 예측된 레이블을 가져옴\n","predicted_label = torch.argmax(logits, dim=1).cpu().numpy()[0]\n","\n","# 예측된 분류 결과 출력\n","class_names = ['민사', '형사']\n","print(\"예측된 분류:\", class_names[predicted_label])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fb5aPuKZNEHD","executionInfo":{"status":"ok","timestamp":1706615408059,"user_tz":-540,"elapsed":4136,"user":{"displayName":"김윤서","userId":"14816156544566013232"}},"outputId":"3a7e67e4-46a0-4045-d0b8-e5468dc1f9a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["예측된 분류: 형사\n"]}]},{"cell_type":"code","source":["import os\n","import json\n","import torch\n","import numpy as np\n","from transformers import BertTokenizer, BertForSequenceClassification\n","\n","# 폴더 내의 모든 파일 경로를 가져오는 함수\n","def get_file_paths(directory):\n","    file_paths = [os.path.join(directory, filename) for filename in os.listdir(directory) if os.path.isfile(os.path.join(directory, filename))]\n","    return file_paths\n","\n","# 'bsisFacts' 값을 추출하는 함수\n","def extract_bsisFacts_from_folder(folder_path):\n","    bsisFacts_list = []\n","    for filename in os.listdir(folder_path):\n","        file_path = os.path.join(folder_path, filename)\n","        if os.path.isfile(file_path) and filename.endswith('.json'):\n","            with open(file_path, 'r', encoding='utf-8') as file:\n","                content = json.load(file)\n","                if 'facts' in content and 'bsisFacts' in content['facts']:\n","                    bsisFacts = content['facts']['bsisFacts']\n","                    bsisFacts_list.extend(bsisFacts)\n","    return bsisFacts_list\n","\n","# 불용어 처리를 수행하는 함수\n","def remove_stopwords(text, stop_words):\n","    tokens = text.split()\n","    filtered_tokens = [word for word in tokens if word not in stop_words]\n","    return ' '.join(filtered_tokens)\n","\n","# 폴더 경로 지정\n","civil_case_directory = '/content/drive/MyDrive/SKT_FLY_AI/Project/civil/'  # 민사 사건 파일이 있는 폴더 경로\n","criminal_case_directory = '/content/drive/MyDrive/SKT_FLY_AI/Project/criminal/'  # 형사 사건 파일이 있는 폴더 경로\n","\n","# 데이터 로드 및 전처리\n","civil_texts = extract_bsisFacts_from_folder(civil_case_directory)\n","criminal_texts = extract_bsisFacts_from_folder(criminal_case_directory)\n","\n","# 사용자 정의 불용어 리스트 정의\n","stop_words = ['는', '의', '에', '을', '를', '과', '와', '이', '가', '도', '으로', '로', '에서', '하다', '형사', '민사', '형법', '민법']\n","\n","# 불용어 제거\n","civil_texts = [remove_stopwords(text, stop_words) for text in civil_texts]\n","criminal_texts = [remove_stopwords(text, stop_words) for text in criminal_texts]\n","\n","texts = civil_texts + criminal_texts  # 전체 텍스트 데이터\n","labels = ['민사'] * len(civil_texts) + ['형사'] * len(criminal_texts)  # 민사: 0, 형사: 1\n","\n","# BERT 토크나이저와 모델 로드\n","model_name = 'bert-base-multilingual-cased'\n","model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","tokenizer = BertTokenizer.from_pretrained(model_name)\n","\n","# device 설정\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# 파일에서 텍스트 데이터를 불러오고 전처리\n","new_example_file_path = '/content/drive/MyDrive/SKT_FLY_AI/Project/98가단28503.json'\n","pred_texts = []\n","with open(new_example_file_path, 'r', encoding='utf-8') as file:\n","    new_example_text = json.load(file)\n","    if 'facts' in new_example_text:\n","        # 'facts' 부분만 문자열로 변환하여 추가\n","        facts_text = json.dumps(new_example_text['facts'], ensure_ascii=False)\n","        pred_texts.append(remove_stopwords(facts_text, stop_words))\n","\n","# BERT 토크나이저를 사용하여 데이터를 인코딩\n","encoded_data = tokenizer.batch_encode_plus(\n","    pred_texts,\n","    add_special_tokens=True,\n","    return_attention_mask=True,\n","    pad_to_max_length=True,\n","    max_length=256,\n","    return_tensors='pt'\n",")\n","\n","input_ids = encoded_data['input_ids'].to(device)\n","attention_mask = encoded_data['attention_mask'].to(device)\n","\n","# 모델을 사용하여 예측\n","model.eval()\n","with torch.no_grad():\n","    outputs = model(input_ids, attention_mask=attention_mask)\n","    logits = outputs.logits\n","\n","# 예측된 레이블을 가져옴\n","predicted_label = torch.argmax(logits, dim=1).cpu().numpy()[0]\n","\n","# 예측된 분류 결과 출력\n","class_names = ['민사', '형사']\n","print(\"예측된 분류:\", class_names[predicted_label])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kGZEHidmHGHx","executionInfo":{"status":"ok","timestamp":1706615532953,"user_tz":-540,"elapsed":5319,"user":{"displayName":"김윤서","userId":"14816156544566013232"}},"outputId":"1c0132fa-e086-4081-86d9-a5076e0ddf14"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["예측된 분류: 민사\n"]}]},{"cell_type":"code","source":["# 잘못 나온 예시\n","\n","import os\n","import json\n","import torch\n","import numpy as np\n","from transformers import BertTokenizer, BertForSequenceClassification\n","\n","# 폴더 내의 모든 파일 경로를 가져오는 함수\n","def get_file_paths(directory):\n","    file_paths = [os.path.join(directory, filename) for filename in os.listdir(directory) if os.path.isfile(os.path.join(directory, filename))]\n","    return file_paths\n","\n","# 'bsisFacts' 값을 추출하는 함수\n","def extract_bsisFacts_from_folder(folder_path):\n","    bsisFacts_list = []\n","    for filename in os.listdir(folder_path):\n","        file_path = os.path.join(folder_path, filename)\n","        if os.path.isfile(file_path) and filename.endswith('.json'):\n","            with open(file_path, 'r', encoding='utf-8') as file:\n","                content = json.load(file)\n","                if 'facts' in content and 'bsisFacts' in content['facts']:\n","                    bsisFacts = content['facts']['bsisFacts']\n","                    bsisFacts_list.extend(bsisFacts)\n","    return bsisFacts_list\n","\n","# 불용어 처리를 수행하는 함수\n","def remove_stopwords(text, stop_words):\n","    tokens = text.split()\n","    filtered_tokens = [word for word in tokens if word not in stop_words]\n","    return ' '.join(filtered_tokens)\n","\n","# 폴더 경로 지정\n","civil_case_directory = '/content/drive/MyDrive/SKT_FLY_AI/Project/civil/'  # 민사 사건 파일이 있는 폴더 경로\n","criminal_case_directory = '/content/drive/MyDrive/SKT_FLY_AI/Project/criminal/'  # 형사 사건 파일이 있는 폴더 경로\n","\n","# 데이터 로드 및 전처리\n","civil_texts = extract_bsisFacts_from_folder(civil_case_directory)\n","criminal_texts = extract_bsisFacts_from_folder(criminal_case_directory)\n","\n","# 사용자 정의 불용어 리스트 정의\n","stop_words = ['는', '의', '에', '을', '를', '과', '와', '이', '가', '도', '으로', '로', '에서', '하다', '형사', '민사', '형법', '민법']\n","\n","# 불용어 제거\n","civil_texts = [remove_stopwords(text, stop_words) for text in civil_texts]\n","criminal_texts = [remove_stopwords(text, stop_words) for text in criminal_texts]\n","\n","texts = civil_texts + criminal_texts  # 전체 텍스트 데이터\n","labels = ['민사'] * len(civil_texts) + ['형사'] * len(criminal_texts)  # 민사: 0, 형사: 1\n","\n","# BERT 토크나이저와 모델 로드\n","model_name = 'bert-base-multilingual-cased'\n","model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","tokenizer = BertTokenizer.from_pretrained(model_name)\n","\n","# device 설정\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# 파일에서 텍스트 데이터를 불러오고 전처리\n","new_example_file_path = '/content/drive/MyDrive/SKT_FLY_AI/Project/98나4361.json'\n","pred_texts = []\n","with open(new_example_file_path, 'r', encoding='utf-8') as file:\n","    new_example_text = json.load(file)\n","    if 'facts' in new_example_text:\n","        # 'facts' 부분만 문자열로 변환하여 추가\n","        facts_text = json.dumps(new_example_text['facts'], ensure_ascii=False)\n","        pred_texts.append(remove_stopwords(facts_text, stop_words))\n","\n","# BERT 토크나이저를 사용하여 데이터를 인코딩\n","encoded_data = tokenizer.batch_encode_plus(\n","    pred_texts,\n","    add_special_tokens=True,\n","    return_attention_mask=True,\n","    pad_to_max_length=True,\n","    max_length=256,\n","    return_tensors='pt'\n",")\n","\n","input_ids = encoded_data['input_ids'].to(device)\n","attention_mask = encoded_data['attention_mask'].to(device)\n","\n","# 모델을 사용하여 예측\n","model.eval()\n","with torch.no_grad():\n","    outputs = model(input_ids, attention_mask=attention_mask)\n","    logits = outputs.logits\n","\n","# 예측된 레이블을 가져옴\n","predicted_label = torch.argmax(logits, dim=1).cpu().numpy()[0]\n","\n","# 예측된 분류 결과 출력\n","class_names = ['민사', '형사']\n","print(\"예측된 분류:\", class_names[predicted_label])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XbCVy8-3NnH2","executionInfo":{"status":"ok","timestamp":1706615556297,"user_tz":-540,"elapsed":3530,"user":{"displayName":"김윤서","userId":"14816156544566013232"}},"outputId":"8ce1972f-660e-4104-9b4e-668ee40aebf4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["예측된 분류: 형사\n"]}]},{"cell_type":"code","source":["import os\n","import json\n","import torch\n","import numpy as np\n","from transformers import BertTokenizer, BertForSequenceClassification\n","\n","# 폴더 내의 모든 파일 경로를 가져오는 함수\n","def get_file_paths(directory):\n","    file_paths = [os.path.join(directory, filename) for filename in os.listdir(directory) if os.path.isfile(os.path.join(directory, filename))]\n","    return file_paths\n","\n","# 'bsisFacts' 값을 추출하는 함수\n","def extract_bsisFacts_from_folder(folder_path):\n","    bsisFacts_list = []\n","    for filename in os.listdir(folder_path):\n","        file_path = os.path.join(folder_path, filename)\n","        if os.path.isfile(file_path) and filename.endswith('.json'):\n","            with open(file_path, 'r', encoding='utf-8') as file:\n","                content = json.load(file)\n","                if 'facts' in content and 'bsisFacts' in content['facts']:\n","                    bsisFacts = content['facts']['bsisFacts']\n","                    bsisFacts_list.extend(bsisFacts)\n","    return bsisFacts_list\n","\n","# 불용어 처리를 수행하는 함수\n","def remove_stopwords(text, stop_words):\n","    tokens = text.split()\n","    filtered_tokens = [word for word in tokens if word not in stop_words]\n","    return ' '.join(filtered_tokens)\n","\n","# 폴더 경로 지정\n","civil_case_directory = '/content/drive/MyDrive/SKT_FLY_AI/Project/civil/'  # 민사 사건 파일이 있는 폴더 경로\n","criminal_case_directory = '/content/drive/MyDrive/SKT_FLY_AI/Project/criminal/'  # 형사 사건 파일이 있는 폴더 경로\n","\n","# 데이터 로드 및 전처리\n","civil_texts = extract_bsisFacts_from_folder(civil_case_directory)\n","criminal_texts = extract_bsisFacts_from_folder(criminal_case_directory)\n","\n","# 사용자 정의 불용어 리스트 정의\n","stop_words = ['는', '의', '에', '을', '를', '과', '와', '이', '가', '도', '으로', '로', '에서', '하다', '형사', '민사', '형법', '민법']\n","\n","# 불용어 제거\n","civil_texts = [remove_stopwords(text, stop_words) for text in civil_texts]\n","criminal_texts = [remove_stopwords(text, stop_words) for text in criminal_texts]\n","\n","texts = civil_texts + criminal_texts  # 전체 텍스트 데이터\n","labels = ['민사'] * len(civil_texts) + ['형사'] * len(criminal_texts)  # 민사: 0, 형사: 1\n","\n","# BERT 토크나이저와 모델 로드\n","model_name = 'bert-base-multilingual-cased'\n","model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","tokenizer = BertTokenizer.from_pretrained(model_name)\n","\n","# device 설정\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# 파일에서 텍스트 데이터를 불러오고 전처리\n","new_example_file_path = '/content/drive/MyDrive/SKT_FLY_AI/Project/2000고단7136.json'\n","pred_texts = []\n","with open(new_example_file_path, 'r', encoding='utf-8') as file:\n","    new_example_text = json.load(file)\n","    if 'facts' in new_example_text:\n","        # 'facts' 부분만 문자열로 변환하여 추가\n","        facts_text = json.dumps(new_example_text['facts'], ensure_ascii=False)\n","        pred_texts.append(remove_stopwords(facts_text, stop_words))\n","\n","# BERT 토크나이저를 사용하여 데이터를 인코딩\n","encoded_data = tokenizer.batch_encode_plus(\n","    pred_texts,\n","    add_special_tokens=True,\n","    return_attention_mask=True,\n","    pad_to_max_length=True,\n","    max_length=256,\n","    return_tensors='pt'\n",")\n","\n","input_ids = encoded_data['input_ids'].to(device)\n","attention_mask = encoded_data['attention_mask'].to(device)\n","\n","# 모델을 사용하여 예측\n","model.eval()\n","with torch.no_grad():\n","    outputs = model(input_ids, attention_mask=attention_mask)\n","    logits = outputs.logits\n","\n","# 예측된 레이블을 가져옴\n","predicted_label = torch.argmax(logits, dim=1).cpu().numpy()[0]\n","\n","# 예측된 분류 결과 출력\n","class_names = ['민사', '형사']\n","print(\"예측된 분류:\", class_names[predicted_label])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aG8RU2LbNxDX","executionInfo":{"status":"ok","timestamp":1706615617610,"user_tz":-540,"elapsed":4055,"user":{"displayName":"김윤서","userId":"14816156544566013232"}},"outputId":"b86e7e68-7e03-444f-e1a6-687092a752c5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["예측된 분류: 형사\n"]}]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V100","mount_file_id":"1REecCG4nxzyDxNGP7ft4yFVNxco99hvX","authorship_tag":"ABX9TyNlVlG2yWm4zMA5e4JwBZAh"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}